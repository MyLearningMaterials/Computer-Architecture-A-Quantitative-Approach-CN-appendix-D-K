# D.4 I/O性能，可靠性措施和基准

I/O性能的度量方法有一些没有在设计中体现。其中之一是**多样性**：哪些I/O设备可以连接到计算机系统?另一个是**容量**：有多少I/O设备可以连接到计算机系统?

除了这些独特的度量之外，传统的性能度量(即响应时间和吞吐量)也适用于I/O。(I/O吞吐量有时称为I/O带宽，响应时间有时称为延迟。)接下来的两个图展示了响应时间和吞吐量是如何对抗和权衡的。图D.8显示了简单的生产者-服务器模型。生产者创建要执行的任务并将它们放在缓冲区中；服务器从先入先出缓冲区接收任务并执行它们。

<figure><img src="../.gitbook/assets/image (2).png" alt=""><figcaption><p>图D.8响应时间和吞吐量的传统生产者-服务器模型。响应时间从任务放入缓冲区时开始，到服务器完成任务时结束。吞吐量是指服务器在单位时间内完成的任务数量。</p></figcaption></figure>

响应时间定义为任务从放入缓冲区到服务器完成任务所花费的时间。吞吐量是服务器在一段时间内完成的任务的平均数量。为了获得尽可能高的吞吐量，服务器不应该处于空闲状态，因此缓冲区不应该为空。

I/O性能的另一个度量是I/O对处理器执行的干扰。传输数据可能会干扰另一个进程的执行。处理I/O中断也有开销。这里我们关心的是一个进程由于I/O被另一个进程占用了多长时间。

### 吞吐量与响应时间

图D.9显示了典型I/O系统的吞吐量与响应时间(或延迟)的关系。曲线的拐点是这样一个区域，在这个区域中，稍微多一点的吞吐量会导致更长的响应时间，或者相反，稍微短一点的响应时间会导致更低的吞吐量。

<figure><img src="../.gitbook/assets/image (1) (1).png" alt=""><figcaption><p>图D.9吞吐量与响应时间的关系。延迟通常报告为响应时间。请注意，最小响应时间仅达到吞吐量的11%，而100%吞吐量的响应时间是最小响应时间的7倍。还要注意，这条曲线中的自变量是隐式的;要跟踪曲线，通常需要改变负载(并发性)。Chen等人[1990]收集了一组磁盘的这些数据。</p></figcaption></figure>

架构师如何平衡这些相互冲突的需求？如果计算机正在与人交互，图D.10给出了一个答案。与计算机的交互或交易分为三个部分:

1. 进入时间——用户输入命令的时间
2. 系统响应时间——用户输入命令到得到完整响应的时间
3. 思考时间——从接收到响应到用户开始输入下一个命令的时间

<figure><img src="../.gitbook/assets/image (2) (1).png" alt=""><figcaption><p>图D.10用户与交互式计算机之间的事务处理，对于传统系统和图形系统，分为输入时间、系统响应时间和用户思考时间。进入时间是相同的，与系统响应时间无关。传统系统的输入时间为4秒，图形系统的输入时间为0.25秒。响应时间的减少实际上减少了事务时间，而不仅仅是响应时间的减少。(摘自布雷迪[1986])</p></figcaption></figure>

这三个部分的总和称为事务时间。一些研究报告称，用户生产力与事务时间成反比。图D.10的结果表明，将系统响应时间缩短0.7秒将比传统事务节省4.9秒(34%)，比图形事务节省2.0秒(70%)。这个令人难以置信的结果可以用人性来解释：当人们得到更快的反应时，他们需要更少的时间来思考。尽管这项研究已经进行了20年，但即使处理器的速度提高了1000倍，响应时间仍然比1秒慢得多。长时间延迟的例子包括由于磁盘I/O过多而在桌面PC上启动应用程序，或者单击Web链接时出现网络延迟。

为了反映响应时间对用户工作效率的重要性，I/O基准测试还处理响应时间与吞吐量之间的权衡。图D.11显示了三个I/O基准的响应时间界限。在给定90%的响应时间必须小于限制或平均响应时间必须小于限制的情况下，它们报告最大吞吐量。

<figure><img src="../.gitbook/assets/image (3).png" alt=""><figcaption><p>图D.11三个I/O基准的响应时间限制</p></figcaption></figure>

### 事务处理基准

事务处理(TP，或联机事务处理的OLTP)主要关注I/O速率(每秒磁盘访问的次数)，而不是数据速率(以每秒传输的字节数衡量)。TP通常涉及对来自许多终端的大量共享信息的更改，TP系统保证故障时的正确行为。例如，假设当客户试图从ATM机中取款时，银行的计算机出现故障。TP系统将保证，如果客户收到钱，账户将被借记，如果客户没有收到钱，账户将保持不变。航空公司预订系统和银行都是TP的传统客户。

正如第1章所提到的，TP社区的24名成员共同制定了行业基准，为了避免法律部门的愤怒，他们匿名发布了这份报告\[Anon. et al 1985]。这份报告催生了交易处理委员会(Transaction Processing Council)，该委员会自成立以来又催生了八个基准。图D.12总结了这些基准。

<figure><img src="../.gitbook/assets/image (8).png" alt=""><figcaption><p>图D.12事务处理委员会基准。总结结果包括性能指标和该指标的性价比。TPC-A、TPC-B、TPC-D、TPC-R退役。</p></figcaption></figure>

让我们来描述一下TPC-C，以了解一下这些基准测试。TPC-C使用数据库模拟批发供应商的订单输入环境，包括输入和发送订单，记录付款，检查订单状态，并监控仓库的库存水平。它运行5个不同复杂性的并发事务，数据库包括9个表，具有可扩展的记录和客户范围。TPC-C以每分钟事务数(tpmC)和系统价格(包括硬件、软件和三年维护支持)来衡量。第1章第42页的图1.17描述了TPC-C在性能和性价比方面的顶级系统。

这些TPC基准是第一个——在某些情况下仍然是唯一一个——具有这些不寻常特征的基准:

* 价格包含在基准结果中。硬件、软件和维护协议的成本包含在提交中，从而可以根据性价比和高性能进行评估。
* 数据集的大小通常必须随着吞吐量的增加而增加。基准测试试图模拟真实的系统，其中对系统的需求和存储在其中的数据的大小一起增加。例如，每分钟有数千人访问数百个银行账户是没有意义的。
* 对基准测试结果进行审计。在提交结果之前，必须经过经过认证的TPC审核员的批准，该审核员执行TPC规则，以确保只提交公平的结果。结果可以受到质疑，争议可以通过向贸易政策委员会提出解决。
* 吞吐量是性能指标，但响应时间是有限的。例如，使用TPC-C, 90%的新订单事务响应时间必须少于5秒。
* 一个独立的组织维护这些基准。TPC收取的会费用于支付包括首席运营办公室在内的行政结构。该组织解决争议，对基准变更的批准进行邮件投票，举行董事会会议，等等。

### SPEC系统级文件服务器，邮件和Web基准

SPEC基准测试工作最出名的是它对处理器性能的描述，但它也为文件服务器、邮件服务器和Web服务器创建了基准测试。

七家公司同意采用一种称为SFS的综合基准来评估运行Sun Microsystems网络文件服务(NFS)的系统。这个基准测试被升级到SFS 3.0(也称为SPEC SFS97\_R1)，以包括对NFS版本3的支持，除了UDP之外还使用TCP作为传输协议，并使混合操作更加真实。对NFS系统的测量导致了读、写和文件操作的综合组合。SFS为比较性能提供默认参数。例如，所有写操作中有一半是在8 KB的块中完成的，一半是在1、2或4 KB的部分块中完成的。对于读，混合是85%的完整块和15%的部分块。

与TPC-C一样，SFS根据报告的吞吐量扩展存储的数据量:每秒每100个NFS操作，容量必须增加1gb。它还限制了平均响应时间，在本例中为40毫秒。图D.13显示了两个NetApp系统的平均响应时间与吞吐量的关系。不幸的是，与TPC基准不同，SFS不会对不同的价格配置进行标准化。

<figure><img src="../.gitbook/assets/image (9).png" alt=""><figcaption><p>图D.13 NetApp FAS3050c NFS服务器两种配置的SPEC SFS97_R1性能。两个处理器达到每秒34,089次操作，四个处理器达到每秒47,927次操作。据2005年5月报道，这些系统使用Data ONTAP 7.0.1R1操作系统，2.8 GHz Pentium Xeon微处理器，每个处理器2gb DRAM，每个系统1gb非易失性存储器，168个15k RPM, 72 GB光纤通道磁盘。这些磁盘通过两个或四个QLogic ISP-2322 FC磁盘控制器连接。</p></figcaption></figure>

SPECMail是帮助评估Internet服务提供商的邮件服务器性能的基准。SPECMail2001基于标准的Internet协议SMTP和POP3，它在将用户数量从10,000扩展到1,000,000时测量吞吐量和用户响应时间。

SPECWeb是评估万维网服务器性能的基准，测量同时用户会话的数量。SPECWeb2005工作负载模拟对Web服务提供者的访问，其中服务器支持多个组织的主页。它有三种工作负载:银行业务(HTTPS)、电子商务(HTTP和HTTPS)和支持(HTTP)。

### 可靠性基准的例子

TPC-C基准测试实际上有可靠性要求。基准系统必须能够处理单个磁盘故障，这意味着在实践中，所有提交者都在其存储系统中运行某些RAID组织。

最近的工作集中在系统容错的有效性上。Brown和Patterson\[2000]提出，可用性可以通过检查系统服务质量指标随时间的变化来衡量，因为故障被注入系统。对于Web服务器，最明显的指标是性能(以每秒满足的请求来衡量)和容错程度(以存储子系统、网络连接拓扑等可以容忍的错误数量来衡量)。

最初的实验注入了单个故障(例如磁盘扇区中的写入错误)，并记录了反映在服务质量指标中的系统行为。本例比较了Linux、Solaris和Windows 2000 Server提供的软件RAID实现。SPECWeb99用于提供工作负载和测量性能。为了注入故障，将软件RAID卷中的一个SCSI磁盘替换为模拟磁盘。它是一台使用SCSI控制器运行软件的PC机，在SCSI总线上的其他设备看来，它就像一个磁盘。磁盘仿真器允许注入故障。注入的故障包括各种瞬时磁盘故障(如可纠正的读取错误)和永久故障(如写入时的磁盘介质故障)。

图D.14显示了各个系统在不同故障情况下的表现。最上面的两个图显示了Linux(左边)和Solaris(右边)。如果在重构完成之前第二个磁盘出现故障，RAID系统可能会丢失数据，因此重构时间越长(MTTR)，可用性就越低。但是，更快的重构意味着应用程序性能下降，因为重构会从运行的应用程序中窃取I/O资源。因此，存在一个策略选择，是在重构期间承受性能损失，还是延长漏洞窗口，从而降低预测的MTTF。

<figure><img src="../.gitbook/assets/image (10).png" alt=""><figcaption><p>图D.14同一台计算机上运行Red Hat 6.0 Linux、Solaris 7和Windows 2000操作系统的软件RAID系统可用性基准图。请注意Linux与Windows和Solaris在重建速度上的不同。y轴是运行SPECWeb99时以每秒点击数为单位的行为。箭头表示插入故障的时间。顶部的行给出了插入故障之前99%的性能置信区间。99%的置信区间意味着，如果变量在这个范围之外，那么出现这个值的概率只有1%。</p></figcaption></figure>

尽管测试的系统都没有在源代码之外记录它们的重构策略，但即使是单个错误注入也能够深入了解这些策略。实验表明，当活动磁盘由于故障而退出服务时，Linux和Solaris都会自动将RAID卷重建到热备盘上。虽然Windows支持RAID重建，必须手动启动重建。因此，在没有人为干预的情况下，在第一次失败后没有重建的Windows系统仍然容易受到第二次失败的影响，这增加了漏洞的窗口。一旦被要求修复，它确实能很快修复。

故障注入实验还提供了对Linux、Solaris和Windows 2000的其他可用性策略的深入了解，这些策略涉及自动空闲利用率、重构率、瞬态错误等等。同样，没有系统记录他们的政策。

在管理瞬时故障方面，故障注入实验表明，Linux的软件RAID实现采用了与Solaris和Windows中的RAID实现相反的方法。Linux实现是偏执狂的——它宁愿在出现第一个错误时以受控的方式关闭磁盘，而不是等待查看错误是否是暂时的。相比之下，Solaris和Windows更加宽容——它们忽略大多数短暂的错误，期望它们不会再发生。因此，这些系统实质上比Linux系统更健壮。请注意，Windows和Solaris都会记录瞬时故障，确保即使没有采取行动也会报告错误。当故障是永久性的，系统表现相似。
